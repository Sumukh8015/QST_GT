{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b06f0691",
      "metadata": {},
      "source": [
        "# Qudit/Qubit–Qutrit Operator Partitioning — Heuristics, Spectral, ILP, and GNN\n",
        "\n",
        "This notebook bundles all code from our snippets into one place. It supports:\n",
        "- **Graph heuristics**: DSATUR, RLF\n",
        "- **Spectral clustering** with refinement\n",
        "- **ILP** (minimum clique cover on commutativity graph) via PuLP/CBC\n",
        "- **GNN**:\n",
        "  - Per-instance unsupervised **Graph Autoencoder (GAE)** that trains and colors a single graph\n",
        "  - Pretrained model: **train on small graphs once → reuse on larger graphs** without retraining\n",
        "- **Hybrid systems**: tensor product of `nq` qubits (d=2) and `nt` qutrits (d=3)\n",
        "\n",
        "> The goal is to partition (color) a **non-commutativity graph** of operators (edges connect *non-commuting* pairs) so that each color class can be measured/executed together; or equivalently, a **clique cover** on the *commutativity* graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f9b125",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install -q numpy networkx pulp\n",
        "# For PyTorch\n",
        "# !pip install -q torch --index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "13bfd11f",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "from time import time\n",
        "\n",
        "# ILP\n",
        "from pulp import (\n",
        "    LpProblem, LpVariable, lpSum, LpMinimize, LpBinary,\n",
        "    LpStatus, value, PULP_CBC_CMD\n",
        ")\n",
        "\n",
        "# Heuristic coloring\n",
        "from networkx.algorithms.coloring import greedy_color\n",
        "\n",
        "# Torch optional\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    import torch.nn.functional as F\n",
        "    TORCH_AVAILABLE = True\n",
        "except Exception:\n",
        "    TORCH_AVAILABLE = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ad72db9d",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def generate_su_d_basis(d: int):\n",
        "    \"\"\"Generate a Hermitian, traceless basis for su(d) (Pauli-like generalization).\n",
        "    Returns a list of dxd complex numpy arrays. Includes symmetric, antisymmetric, and diagonal gens.\n",
        "    \"\"\"\n",
        "    basis = []\n",
        "    for i in range(d):\n",
        "        for j in range(i + 1, d):\n",
        "            mat = np.zeros((d, d), dtype=complex)\n",
        "            mat[i, j] = 1\n",
        "            mat[j, i] = 1\n",
        "            basis.append(mat)\n",
        "\n",
        "            mat = np.zeros((d, d), dtype=complex)\n",
        "            mat[i, j] = -1j\n",
        "            mat[j, i] = 1j\n",
        "            basis.append(mat)\n",
        "\n",
        "    for i in range(1, d):\n",
        "        diag = np.zeros((d, d), dtype=complex)\n",
        "        for j in range(i):\n",
        "            diag[j, j] = 1\n",
        "        diag[i, i] = -i\n",
        "        diag /= np.sqrt(i * (i + 1))\n",
        "        basis.append(diag)\n",
        "\n",
        "    return basis\n",
        "\n",
        "\n",
        "def tensor_product_operators(single_site_ops, N: int):\n",
        "    from itertools import product\n",
        "    ops = []\n",
        "    labels = []\n",
        "    basis_size = len(single_site_ops)\n",
        "    for idxs in product(range(basis_size), repeat=N):\n",
        "        label = \"-\".join(map(str, idxs))\n",
        "        op = single_site_ops[idxs[0]]\n",
        "        for i in idxs[1:]:\n",
        "            op = np.kron(op, single_site_ops[i])\n",
        "        ops.append(op)\n",
        "        labels.append(label)\n",
        "    return ops, labels\n",
        "\n",
        "\n",
        "def generate_non_commutativity_graph(ops):\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(len(ops)))\n",
        "    for i, j in combinations(range(len(ops)), 2):\n",
        "        if not np.allclose(ops[i] @ ops[j], ops[j] @ ops[i]):\n",
        "            G.add_edge(i, j)\n",
        "    return G\n",
        "\n",
        "\n",
        "def generate_commutativity_graph(ops):\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(len(ops)))\n",
        "    for i, j in combinations(range(len(ops)), 2):\n",
        "        if np.allclose(ops[i] @ ops[j], ops[j] @ ops[i]):\n",
        "            G.add_edge(i, j)\n",
        "    return G\n",
        "\n",
        "\n",
        "# ---- coloring helpers ---- #\n",
        "def coloring_is_valid(coloring: dict, G: nx.Graph) -> bool:\n",
        "    for u, v in G.edges():\n",
        "        if coloring.get(u) == coloring.get(v):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def relabel_coloring_sequential(coloring: dict) -> dict:\n",
        "    \"\"\"Map color labels to 0..K-1 consistently.\"\"\"\n",
        "    uniq = sorted(set(coloring.values()))\n",
        "    remap = {c: i for i, c in enumerate(uniq)}\n",
        "    return {n: remap[c] for n, c in coloring.items()}\n",
        "\n",
        "\n",
        "def refine_by_greedy_within_clusters(prelim: dict, G: nx.Graph) -> dict:\n",
        "    \"\"\"Within each preliminary cluster label, run greedy coloring on the induced subgraph\n",
        "    and offset color ids to keep them disjoint; then merge.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    offset = 0\n",
        "    for label in sorted(set(prelim.values())):\n",
        "        nodes = [n for n, lab in prelim.items() if lab == label]\n",
        "        H = G.subgraph(nodes).copy()\n",
        "        local = greedy_color(H, strategy=\"saturation_largest_first\")\n",
        "        # shift\n",
        "        for n in nodes:\n",
        "            out[n] = local[n] + offset\n",
        "        offset = max(out.values()) + 1 if out else 0\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "05165ae3",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def dsatur_color(G: nx.Graph):\n",
        "    coloring = greedy_color(G, strategy=\"saturation_largest_first\")\n",
        "    k = len(set(coloring.values())) if coloring else 0\n",
        "    return coloring, k\n",
        "\n",
        "\n",
        "def rlf_color(G: nx.Graph):\n",
        "    coloring = greedy_color(G, strategy=\"largest_first\")\n",
        "    k = len(set(coloring.values())) if coloring else 0\n",
        "    return coloring, k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2ff69a6a",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def _adjacency_matrix(G: nx.Graph) -> np.ndarray:\n",
        "    n = G.number_of_nodes()\n",
        "    A = np.zeros((n, n), dtype=float)\n",
        "    for u, v in G.edges():\n",
        "        A[u, v] = 1.0\n",
        "        A[v, u] = 1.0\n",
        "    return A\n",
        "\n",
        "\n",
        "def _kmeans_numpy(X: np.ndarray, k: int, rng: np.random.Generator, n_init: int = 10, max_iter: int = 100) -> np.ndarray:\n",
        "    n, d = X.shape\n",
        "    best_inertia = np.inf\n",
        "    best_labels = None\n",
        "    for _ in range(max(1, n_init)):\n",
        "        centers = np.empty((k, d))\n",
        "        # first center\n",
        "        idx0 = int(rng.integers(0, n))\n",
        "        centers[0] = X[idx0]\n",
        "        # rest via approximate k-means++\n",
        "        closest = np.full(n, np.inf)\n",
        "        for ci in range(1, k):\n",
        "            dist_sq = ((X[:, None, :] - centers[None, :ci, :]) ** 2).sum(axis=2).min(axis=1)\n",
        "            closest = np.minimum(closest, dist_sq)\n",
        "            denom = float(closest.sum()) or 1.0\n",
        "            probs = closest / denom\n",
        "            next_idx = int(rng.choice(n, p=probs))\n",
        "            centers[ci] = X[next_idx]\n",
        "        labels = np.zeros(n, dtype=int)\n",
        "        for _it in range(max_iter):\n",
        "            dists = ((X[:, None, :] - centers[None, :, :]) ** 2).sum(axis=2)\n",
        "            new_labels = dists.argmin(axis=1)\n",
        "            if np.array_equal(new_labels, labels):\n",
        "                break\n",
        "            labels = new_labels\n",
        "            for j in range(k):\n",
        "                mask = labels == j\n",
        "                if np.any(mask):\n",
        "                    centers[j] = X[mask].mean(axis=0)\n",
        "                else:\n",
        "                    centers[j] = X[int(rng.integers(0, n))]\n",
        "        # inertia\n",
        "        inertia = 0.0\n",
        "        for j in range(k):\n",
        "            mask = labels == j\n",
        "            if np.any(mask):\n",
        "                inertia += ((X[mask] - centers[j]) ** 2).sum()\n",
        "        if inertia < best_inertia:\n",
        "            best_inertia = inertia\n",
        "            best_labels = labels.copy()\n",
        "    return best_labels\n",
        "\n",
        "\n",
        "def spectral_coloring(G: nx.Graph, upper_bound_k: int, rng: np.random.Generator):\n",
        "    \"\"\"Use Laplacian eigen-embeddings + k-means to propose clusters; then refine.\n",
        "\n",
        "    Tries k = 2..upper_bound_k and returns the first valid coloring. If none valid, falls back to greedy.\n",
        "\n",
        "    Returns (coloring_dict, num_colors, elapsed_sec).\n",
        "    \"\"\"\n",
        "    start = time()\n",
        "    n = G.number_of_nodes()\n",
        "    if n == 0:\n",
        "        return {}, 0, 0.0\n",
        "    if upper_bound_k <= 1:\n",
        "        col = {i: 0 for i in G.nodes()}\n",
        "        return col, 1, 0.0\n",
        "\n",
        "    A = _adjacency_matrix(G)\n",
        "    D = np.diag(A.sum(axis=1))\n",
        "    L = D - A\n",
        "    # compute a bunch of eigenvectors of L (smallest eigenvalues)\n",
        "    # to be safe, ask for up to min(upper_bound_k, n) eigenvectors\n",
        "    m = int(min(max(2, upper_bound_k), n))\n",
        "    w, V = np.linalg.eigh(L)  # full since graphs are small/moderate\n",
        "    # take the m smallest eigenvectors (skip the first all-ones if present)\n",
        "    X = V[:, :m]\n",
        "\n",
        "    best = None\n",
        "    best_k = None\n",
        "    for k in range(2, max(2, upper_bound_k) + 1):\n",
        "        labels = _kmeans_numpy(X, k=k, rng=rng, n_init=5, max_iter=100)\n",
        "        prelim = {i: int(labels[i]) for i in range(n)}\n",
        "        refined = refine_by_greedy_within_clusters(prelim, G)\n",
        "        if coloring_is_valid(refined, G):\n",
        "            best = refined\n",
        "            best_k = len(set(refined.values()))\n",
        "            break\n",
        "\n",
        "    if best is None:\n",
        "        # fallback: greedy DSATUR\n",
        "        prelim = greedy_color(G, strategy=\"saturation_largest_first\")\n",
        "        best = prelim\n",
        "        best_k = len(set(prelim.values()))\n",
        "    return relabel_coloring_sequential(best), best_k, time() - start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c412fdff",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def solve_ilp_clique_cover(comm_graph: nx.Graph):\n",
        "    cliques = list(nx.find_cliques(comm_graph))\n",
        "    prob = LpProblem(\"MinCliqueCover\", LpMinimize)\n",
        "    vars = [LpVariable(f\"c{i}\", cat=LpBinary) for i in range(len(cliques))]\n",
        "    prob += lpSum(vars)\n",
        "    for v in comm_graph.nodes:\n",
        "        prob += lpSum(vars[i] for i, clique in enumerate(cliques) if v in clique) >= 1\n",
        "    start = time()\n",
        "    prob.solve(PULP_CBC_CMD(msg=0))\n",
        "    elapsed = time() - start\n",
        "    selected_cliques = [cliques[i] for i in range(len(cliques)) if value(vars[i]) > 0.5]\n",
        "    node_to_color = {}\n",
        "    for color, clique in enumerate(selected_cliques):\n",
        "        for node in clique:\n",
        "            if node not in node_to_color:\n",
        "                node_to_color[node] = color\n",
        "    return node_to_color, elapsed, LpStatus[prob.status]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4891806f",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "# ---- GNN model and IO ---- #\n",
        "def normalized_adj_with_selfloops(A: np.ndarray):\n",
        "    A_sl = A + np.eye(A.shape[0])\n",
        "    d = A_sl.sum(axis=1)\n",
        "    with np.errstate(divide='ignore'):\n",
        "        d_inv_sqrt = 1.0 / np.sqrt(np.maximum(d, 1e-12))\n",
        "    D_inv_sqrt = np.diag(d_inv_sqrt)\n",
        "    return D_inv_sqrt @ A_sl @ D_inv_sqrt\n",
        "\n",
        "\n",
        "def make_node_features(A: np.ndarray):\n",
        "    deg = A.sum(axis=1, keepdims=True)\n",
        "    ones = np.ones_like(deg)\n",
        "    X = np.hstack([deg / max(1.0, float(deg.max() or 1.0)), ones])\n",
        "    return X.astype(np.float32)\n",
        "\n",
        "\n",
        "class GCNLayer(nn.Module if TORCH_AVAILABLE else object):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        if not TORCH_AVAILABLE: return\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_dim, out_dim, bias=False)\n",
        "\n",
        "    def forward(self, X, A_hat):\n",
        "        return A_hat @ self.lin(X)\n",
        "\n",
        "\n",
        "class GAE(nn.Module if TORCH_AVAILABLE else object):\n",
        "    def __init__(self, in_dim=2, hidden_dim=32, emb_dim=16):\n",
        "        if not TORCH_AVAILABLE: return\n",
        "        super().__init__()\n",
        "        self.gcn1 = GCNLayer(in_dim, hidden_dim)\n",
        "        self.gcn2 = GCNLayer(hidden_dim, emb_dim)\n",
        "\n",
        "    def forward(self, X, A_hat):\n",
        "        Z = self.gcn1(X, A_hat)\n",
        "        Z = F.relu(Z)\n",
        "        Z = self.gcn2(Z, A_hat)\n",
        "        return Z\n",
        "\n",
        "\n",
        "def build_inputs_from_graph(G: nx.Graph, device='cpu'):\n",
        "    if not TORCH_AVAILABLE:\n",
        "        raise RuntimeError(\"PyTorch is required for GNN training/inference.\")\n",
        "    A = _adjacency_matrix(G)\n",
        "    A_hat = normalized_adj_with_selfloops(A).astype(np.float32)\n",
        "    X = make_node_features(A)\n",
        "    t_X = torch.from_numpy(X).to(device)\n",
        "    t_Ahat = torch.from_numpy(A_hat).to(device)\n",
        "    return t_X, t_Ahat\n",
        "\n",
        "\n",
        "def save_model(model, path: str, in_dim: int, hidden_dim: int, emb_dim: int):\n",
        "    if not TORCH_AVAILABLE:\n",
        "        raise RuntimeError(\"PyTorch not available; cannot save model.\")\n",
        "    payload = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'config': {'in_dim': in_dim, 'hidden_dim': hidden_dim, 'emb_dim': emb_dim},\n",
        "    }\n",
        "    torch.save(payload, path)\n",
        "\n",
        "\n",
        "def load_model(path: str, device='cpu'):\n",
        "    if not TORCH_AVAILABLE:\n",
        "        raise RuntimeError(\"PyTorch not available; cannot load model.\")\n",
        "    payload = torch.load(path, map_location=device)\n",
        "    cfg = payload['config']\n",
        "    model = GAE(in_dim=cfg['in_dim'], hidden_dim=cfg['hidden_dim'], emb_dim=cfg['emb_dim'])\n",
        "    model.load_state_dict(payload['state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model, cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e407d53c",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def _sample_neg_edges(n: int, A: np.ndarray, num_samples: int, rng: np.random.Generator) -> np.ndarray:\n",
        "    neg = []\n",
        "    tries = 0\n",
        "    limit = max(10 * num_samples, 100)\n",
        "    while len(neg) < num_samples and tries < limit:\n",
        "        i = int(rng.integers(0, n)); j = int(rng.integers(0, n))\n",
        "        if i == j: \n",
        "            tries += 1; \n",
        "            continue\n",
        "        u, v = (i, j) if i < j else (j, i)\n",
        "        if A[u, v] == 0:\n",
        "            neg.append((u, v))\n",
        "        tries += 1\n",
        "    if len(neg) < num_samples:\n",
        "        for u in range(n):\n",
        "            for v in range(u + 1, n):\n",
        "                if A[u, v] == 0 and len(neg) < num_samples:\n",
        "                    neg.append((u, v))\n",
        "    return np.array(neg, dtype=int)\n",
        "\n",
        "\n",
        "def gnn_coloring(noncomm_graph: nx.Graph, upper_bound_k: int, rng: np.random.Generator,\n",
        "                 epochs: int = 200, hidden_dim: int = 32, emb_dim: int = 16, lr: float = 1e-2,\n",
        "                 verbose: bool = False):\n",
        "    \"\"\"Per-instance unsupervised GAE: trains on the given graph, outputs a coloring.\n",
        "\n",
        "    Returns (coloring_dict, num_colors, elapsed_sec, used_gnn: bool).\n",
        "    \"\"\"\n",
        "    start_total = time()\n",
        "    n = noncomm_graph.number_of_nodes()\n",
        "    if n == 0:\n",
        "        return {}, 0, 0.0, False\n",
        "    if upper_bound_k <= 1:\n",
        "        return {i: 0 for i in noncomm_graph.nodes()}, 1, 0.0, False\n",
        "    if not TORCH_AVAILABLE:\n",
        "        col, k, t = spectral_coloring(noncomm_graph, upper_bound_k, rng)\n",
        "        return col, k, t, False\n",
        "\n",
        "    device = torch.device('cpu')\n",
        "    t_X, t_Ahat = build_inputs_from_graph(noncomm_graph, device=device)\n",
        "\n",
        "    A = _adjacency_matrix(noncomm_graph)\n",
        "    pos_pairs = np.array([(u, v) if u < v else (v, u) for u, v in noncomm_graph.edges()], dtype=int)\n",
        "    num_pos = len(pos_pairs)\n",
        "    neg_pairs = _sample_neg_edges(n, A, num_pos, rng)\n",
        "\n",
        "    t_pos = torch.from_numpy(pos_pairs).long().to(device)\n",
        "    t_neg = torch.from_numpy(neg_pairs).long().to(device)\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    model = GAE(in_dim=t_X.shape[1], hidden_dim=hidden_dim, emb_dim=emb_dim).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    model.train()\n",
        "    for ep in range(epochs):\n",
        "        opt.zero_grad()\n",
        "        Z = model(t_X, t_Ahat)\n",
        "        pos_scores = (Z[t_pos[:, 0]] * Z[t_pos[:, 1]]).sum(dim=1)\n",
        "        neg_scores = (Z[t_neg[:, 0]] * Z[t_neg[:, 1]]).sum(dim=1)\n",
        "        loss = bce(pos_scores, torch.ones_like(pos_scores)) + bce(neg_scores, torch.zeros_like(neg_scores))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        if verbose and ((ep + 1) % 50 == 0 or ep == 0):\n",
        "            print(f\"[GNN] epoch {ep+1:03d} loss={loss.item():.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Z = model(t_X, t_Ahat).cpu().numpy()\n",
        "\n",
        "    best, best_k = None, None\n",
        "    for k in range(2, max(2, upper_bound_k) + 1):\n",
        "        labels = _kmeans_numpy(Z, k=k, rng=rng, n_init=5, max_iter=100)\n",
        "        prelim = {i: int(labels[i]) for i in range(n)}\n",
        "        refined = refine_by_greedy_within_clusters(prelim, noncomm_graph)\n",
        "        if coloring_is_valid(refined, noncomm_graph):\n",
        "            best = refined\n",
        "            best_k = len(set(refined.values()))\n",
        "            break\n",
        "    if best is None:\n",
        "        prelim = greedy_color(noncomm_graph, strategy=\"saturation_largest_first\")\n",
        "        best = prelim\n",
        "        best_k = len(set(prelim.values()))\n",
        "    return relabel_coloring_sequential(best), best_k, time() - start_total, True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "19bb5930",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def _graph_from_random_ops(d, N, M, seed):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    identity = np.eye(d, dtype=complex)\n",
        "    su = generate_su_d_basis(d)\n",
        "    single_site_ops = [identity] + su\n",
        "    ops, _ = tensor_product_operators(single_site_ops, N)\n",
        "    dim = d ** N\n",
        "    I = np.eye(dim)\n",
        "    ops = [op for op in ops if not np.allclose(op, I)]\n",
        "    idx = rng.choice(len(ops), size=M, replace=False)\n",
        "    ops_sel = [ops[i] for i in idx]\n",
        "    G = generate_non_commutativity_graph(ops_sel)\n",
        "    return G\n",
        "\n",
        "\n",
        "def train_gnn_small(out_path: str, graphs: int = 50, d: int = 3, N: int = 1, M: int = 12,\n",
        "                    seed0: int = 0, epochs: int = 300, hidden: int = 64, emb: int = 32, lr: float = 1e-2,\n",
        "                    pairs_per_graph: int = 512):\n",
        "    \"\"\"Train a GAE on many small random graphs and save weights for reuse.\"\"\"\n",
        "    if not TORCH_AVAILABLE:\n",
        "        raise RuntimeError(\"PyTorch not available: cannot train GNN.\")\n",
        "    device = torch.device('cpu')\n",
        "    in_dim = 2\n",
        "    model = GAE(in_dim=in_dim, hidden_dim=hidden, emb_dim=emb).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    rng = np.random.default_rng(seed0)\n",
        "    train_graphs = [_graph_from_random_ops(d, N, M, seed0 + i) for i in range(graphs)]\n",
        "    print(f\"Training on {len(train_graphs)} graphs, epochs={epochs}, pairs/graph={pairs_per_graph}\")\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for G in train_graphs:\n",
        "            t_X, t_Ahat = build_inputs_from_graph(G, device=device)\n",
        "            A = _adjacency_matrix(G)\n",
        "            # pos/neg sampling\n",
        "            pos = np.array([(u, v) if u < v else (v, u) for u, v in G.edges()], dtype=int)\n",
        "            neg = _sample_neg_edges(G.number_of_nodes(), A, max(1, min(pairs_per_graph, len(pos))), rng)\n",
        "            if len(pos) == 0 or len(neg) == 0:\n",
        "                continue\n",
        "            if len(pos) > pairs_per_graph:\n",
        "                pos = pos[:pairs_per_graph]\n",
        "            t_pos = torch.from_numpy(pos).long().to(device)\n",
        "            t_neg = torch.from_numpy(neg).long().to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            Z = model(t_X, t_Ahat)\n",
        "            pos_scores = (Z[t_pos[:, 0]] * Z[t_pos[:, 1]]).sum(dim=1)\n",
        "            neg_scores = (Z[t_neg[:, 0]] * Z[t_neg[:, 1]]).sum(dim=1)\n",
        "            loss = bce(pos_scores, torch.ones_like(pos_scores)) + bce(neg_scores, torch.zeros_like(neg_scores))\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "        if ep % 20 == 0 or ep == 1:\n",
        "            print(f\"Epoch {ep:04d}/{epochs} | loss={total_loss:.4f}\")\n",
        "    save_model(model, out_path, in_dim=in_dim, hidden_dim=hidden, emb_dim=emb)\n",
        "    print(f\"Saved trained GNN to: {out_path}\")\n",
        "\n",
        "\n",
        "def color_with_trained_gnn(noncomm_graph: nx.Graph, model_path: str, upper_bound_k: int, rng, device='cpu'):\n",
        "    if not TORCH_AVAILABLE:\n",
        "        raise RuntimeError(\"PyTorch not available: cannot run GNN inference.\")\n",
        "    model, cfg = load_model(model_path, device=device)\n",
        "    t_X, t_Ahat = build_inputs_from_graph(noncomm_graph, device=device)\n",
        "    with torch.no_grad():\n",
        "        Z = model(t_X, t_Ahat).cpu().numpy()\n",
        "    n = noncomm_graph.number_of_nodes()\n",
        "    if n == 0:\n",
        "        return {}, 0, 0.0\n",
        "    best, best_k = None, None\n",
        "    for k in range(2, max(2, upper_bound_k) + 1):\n",
        "        labels = _kmeans_numpy(Z, k=k, rng=rng, n_init=5, max_iter=100)\n",
        "        prelim = {i: int(labels[i]) for i in range(n)}\n",
        "        refined = refine_by_greedy_within_clusters(prelim, noncomm_graph)\n",
        "        if coloring_is_valid(refined, noncomm_graph):\n",
        "            best = refined\n",
        "            best_k = len(set(refined.values()))\n",
        "            break\n",
        "    if best is None:\n",
        "        prelim = greedy_color(noncomm_graph, strategy=\"saturation_largest_first\")\n",
        "        best = prelim\n",
        "        best_k = len(set(prelim.values()))\n",
        "    return relabel_coloring_sequential(best), best_k, 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "06578b15",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def _single_site_ops(d: int):\n",
        "    I = np.eye(d, dtype=complex)\n",
        "    return [I] + generate_su_d_basis(d)\n",
        "\n",
        "\n",
        "def _build_local_sets(nq: int, nt: int):\n",
        "    qubit_ops = _single_site_ops(2)\n",
        "    qutrit_ops = _single_site_ops(3)\n",
        "    local_sets = [qubit_ops for _ in range(nq)] + [qutrit_ops for _ in range(nt)]\n",
        "    return local_sets\n",
        "\n",
        "\n",
        "def _tensor_from_indices(local_sets, idxs):\n",
        "    op = local_sets[0][idxs[0]]\n",
        "    for s, i in zip(local_sets[1:], idxs[1:]):\n",
        "        op = np.kron(op, s[i])\n",
        "    return op\n",
        "\n",
        "\n",
        "def _sample_unique_ops(local_sets, M: int, rng: np.random.Generator):\n",
        "    arities = [len(s) for s in local_sets]\n",
        "    seen = set()\n",
        "    ops, labels = [], []\n",
        "    max_trials = M * 50\n",
        "    trials = 0\n",
        "    while len(ops) < M and trials < max_trials:\n",
        "        idxs = tuple(int(rng.integers(0, a)) for a in arities)\n",
        "        if all(i == 0 for i in idxs):  # exclude global identity\n",
        "            trials += 1; continue\n",
        "        if idxs in seen:\n",
        "            trials += 1; continue\n",
        "        seen.add(idxs)\n",
        "        op = _tensor_from_indices(local_sets, idxs)\n",
        "        ops.append(op)\n",
        "        nq = sum(1 for s in local_sets if s[0].shape[0] == 2)\n",
        "        parts = []\n",
        "        for p, i in enumerate(idxs[:nq]):\n",
        "            parts.append(f\"q{p}:{i}\")\n",
        "        for p, i in enumerate(idxs[nq:]):\n",
        "            parts.append(f\"t{p}:{i}\")\n",
        "        labels.append(\"|\".join(parts))\n",
        "        trials += 1\n",
        "    if len(ops) < M:\n",
        "        raise RuntimeError(f\"Could only sample {len(ops)} unique operators (requested M={M}). Try reducing M.\")\n",
        "    return ops, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "edd8ef36",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def _parse_methods(methods):\n",
        "    if methods is None: return {'dsatur','rlf','spectral','gnn','ilp'}\n",
        "    if isinstance(methods, str):\n",
        "        tokens = [t.strip().lower() for t in methods.split(',') if t.strip()]\n",
        "    else:\n",
        "        tokens = [str(t).lower().strip() for t in methods]\n",
        "    valid = {'dsatur','rlf','spectral','gnn','ilp','all'}\n",
        "    for t in tokens:\n",
        "        if t not in valid:\n",
        "            raise ValueError(f\"Unknown method '{t}'. Valid: dsatur, rlf, spectral, gnn, ilp, all\")\n",
        "    if not tokens or 'all' in tokens:\n",
        "        return {'dsatur','rlf','spectral','gnn','ilp'}\n",
        "    return set(tokens)\n",
        "\n",
        "\n",
        "def run_benchmark(d=3, N=1, M=8, seed=2, methods='all',\n",
        "                  gnn_epochs=150, gnn_hidden=32, gnn_emb=16, gnn_lr=1e-2,\n",
        "                  gnn_model_path=None, verbose=True):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    selected = _parse_methods(methods)\n",
        "\n",
        "    need_noncomm = bool(selected & {'dsatur','rlf','spectral','gnn'})\n",
        "    need_comm = ('ilp' in selected)\n",
        "\n",
        "    identity = np.eye(d, dtype=complex)\n",
        "    su_d_basis = generate_su_d_basis(d)\n",
        "    single_site_ops = [identity] + su_d_basis\n",
        "    ops, labels = tensor_product_operators(single_site_ops, N)\n",
        "\n",
        "    dim = d ** N\n",
        "    I_global = np.eye(dim)\n",
        "    ops_filtered = [op for op in ops if not np.allclose(op, I_global)]\n",
        "\n",
        "    if M > len(ops_filtered):\n",
        "        raise ValueError(f\"Cannot select M={M}; only {len(ops_filtered)} available after filtering.\")\n",
        "\n",
        "    idx = rng.choice(len(ops_filtered), size=M, replace=False)\n",
        "    ops_sel = [ops_filtered[i] for i in idx]\n",
        "\n",
        "    graphs_info = {'comm_edges':0,'noncomm_edges':0,'comm_build_time':0.0,'noncomm_build_time':0.0}\n",
        "    noncomm_graph = comm_graph = None\n",
        "\n",
        "    if need_noncomm:\n",
        "        start = time(); noncomm_graph = generate_non_commutativity_graph(ops_sel)\n",
        "        graphs_info['noncomm_build_time'] = time() - start\n",
        "        graphs_info['noncomm_edges'] = noncomm_graph.number_of_edges()\n",
        "    if need_comm:\n",
        "        start = time(); comm_graph = generate_commutativity_graph(ops_sel)\n",
        "        graphs_info['comm_build_time'] = time() - start\n",
        "        graphs_info['comm_edges'] = comm_graph.number_of_edges()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Operators: M={M}, d={d}, N={N}\")\n",
        "        if need_noncomm:\n",
        "            print(f\"Non-comm edges={graphs_info['noncomm_edges']}  build={graphs_info['noncomm_build_time']:.6f}s\")\n",
        "        if need_comm:\n",
        "            print(f\"Comm     edges={graphs_info['comm_edges']}      build={graphs_info['comm_build_time']:.6f}s\")\n",
        "\n",
        "    methods_out = {}\n",
        "    ds_k = None\n",
        "\n",
        "    if 'dsatur' in selected and need_noncomm:\n",
        "        t0 = time(); ds_col, ds_k = dsatur_color(noncomm_graph)\n",
        "        methods_out['DSATUR'] = {'colors': ds_k, 'time': time()-t0}\n",
        "    if 'rlf' in selected and need_noncomm:\n",
        "        t0 = time(); rlf_col, rlf_k = rlf_color(noncomm_graph)\n",
        "        methods_out['RLF'] = {'colors': rlf_k, 'time': time()-t0}\n",
        "    if 'spectral' in selected and need_noncomm:\n",
        "        base_ub = max(2, noncomm_graph.number_of_nodes())\n",
        "        ub = ds_k if (ds_k is not None and ds_k > 0) else base_ub\n",
        "        spec_col, spec_k, spec_t = spectral_coloring(noncomm_graph, upper_bound_k=ub, rng=rng)\n",
        "        methods_out['Spectral'] = {'colors': spec_k, 'time': spec_t}\n",
        "    if 'gnn' in selected and need_noncomm:\n",
        "        base_ub = max(2, noncomm_graph.number_of_nodes())\n",
        "        ub = ds_k if (ds_k is not None and ds_k > 0) else base_ub\n",
        "        if gnn_model_path:\n",
        "            try:\n",
        "                gnn_col, gnn_k, gnn_t = color_with_trained_gnn(noncomm_graph, model_path=gnn_model_path, upper_bound_k=ub, rng=rng)\n",
        "                methods_out['GNN'] = {'colors': gnn_k, 'time': gnn_t, 'backend': 'pretrained'}\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] pretrained GNN failed ({e}); falling back to per-instance training.\")\n",
        "                gnn_col, gnn_k, gnn_t, used = gnn_coloring(noncomm_graph, upper_bound_k=ub, rng=rng,\n",
        "                                                          epochs=gnn_epochs, hidden_dim=gnn_hidden,\n",
        "                                                          emb_dim=gnn_emb, lr=gnn_lr, verbose=False)\n",
        "                methods_out['GNN'] = {'colors': gnn_k, 'time': gnn_t, 'backend': 'torch' if used else 'spectral_fallback'}\n",
        "        else:\n",
        "            gnn_col, gnn_k, gnn_t, used = gnn_coloring(noncomm_graph, upper_bound_k=ub, rng=rng,\n",
        "                                                       epochs=gnn_epochs, hidden_dim=gnn_hidden,\n",
        "                                                       emb_dim=gnn_emb, lr=gnn_lr, verbose=False)\n",
        "            methods_out['GNN'] = {'colors': gnn_k, 'time': gnn_t, 'backend': 'torch' if used else 'spectral_fallback'}\n",
        "    if 'ilp' in selected and need_comm:\n",
        "        ilp_col, ilp_t, ilp_status = solve_ilp_clique_cover(comm_graph)\n",
        "        ilp_k = len(set(ilp_col.values()))\n",
        "        methods_out['ILP'] = {'colors': ilp_k, 'time': ilp_t, 'status': ilp_status}\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== Results ===\")\n",
        "        for name in ['DSATUR','RLF','Spectral','GNN','ILP']:\n",
        "            if name in methods_out:\n",
        "                line = f\"{name:8s} colors={methods_out[name].get('colors')} time={methods_out[name].get('time'):.4f}s\"\n",
        "                if name == 'GNN' and 'backend' in methods_out[name]:\n",
        "                    line += f\" backend={methods_out[name]['backend']}\"\n",
        "                if name == 'ILP' and 'status' in methods_out[name]:\n",
        "                    line += f\" status={methods_out[name]['status']}\"\n",
        "                print(line)\n",
        "\n",
        "    return {\n",
        "        'meta': {'d': d, 'N': N, 'M': M, 'seed': seed, 'methods': sorted(list(selected))},\n",
        "        'graphs': graphs_info,\n",
        "        'methods': methods_out,\n",
        "    }\n",
        "\n",
        "\n",
        "def hetero_run_benchmark(nq=1, nt=1, M=20, seed=42, methods='all',\n",
        "                         gnn_epochs=150, gnn_hidden=32, gnn_emb=16, gnn_lr=1e-2,\n",
        "                         gnn_model_path=None, verbose=True):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    selected = _parse_methods(methods)\n",
        "    need_noncomm = bool(selected & {'dsatur','rlf','spectral','gnn'})\n",
        "    need_comm = ('ilp' in selected)\n",
        "\n",
        "    local_sets = _build_local_sets(nq, nt)\n",
        "    ops_sel, labels_sel = _sample_unique_ops(local_sets, M, rng)\n",
        "\n",
        "    graphs_info = {'comm_edges':0, 'noncomm_edges':0, 'comm_build_time':0.0, 'noncomm_build_time':0.0,\n",
        "                   'n_qubits': nq, 'n_qutrits': nt, 'hilbert_dim': int((2**nq)*(3**nt))}\n",
        "    noncomm_graph = comm_graph = None\n",
        "    if need_noncomm:\n",
        "        start = time(); noncomm_graph = generate_non_commutativity_graph(ops_sel)\n",
        "        graphs_info['noncomm_build_time'] = time() - start\n",
        "        graphs_info['noncomm_edges'] = noncomm_graph.number_of_edges()\n",
        "    if need_comm:\n",
        "        start = time(); comm_graph = generate_commutativity_graph(ops_sel)\n",
        "        graphs_info['comm_build_time'] = time() - start\n",
        "        graphs_info['comm_edges'] = comm_graph.number_of_edges()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Hetero system: nq={nq}, nt={nt}, dim={graphs_info['hilbert_dim']}, M={M}\")\n",
        "        if need_noncomm:\n",
        "            print(f\"Non-comm edges={graphs_info['noncomm_edges']}  build={graphs_info['noncomm_build_time']:.6f}s\")\n",
        "        if need_comm:\n",
        "            print(f\"Comm     edges={graphs_info['comm_edges']}      build={graphs_info['comm_build_time']:.6f}s\")\n",
        "\n",
        "    methods_out = {}\n",
        "    ds_k = None\n",
        "    if 'dsatur' in selected and need_noncomm:\n",
        "        t0 = time(); ds_col, ds_k = dsatur_color(noncomm_graph)\n",
        "        methods_out['DSATUR'] = {'colors': ds_k, 'time': time()-t0}\n",
        "    if 'rlf' in selected and need_noncomm:\n",
        "        t0 = time(); rlf_col, rlf_k = rlf_color(noncomm_graph)\n",
        "        methods_out['RLF'] = {'colors': rlf_k, 'time': time()-t0}\n",
        "    if 'spectral' in selected and need_noncomm:\n",
        "        base_ub = max(2, noncomm_graph.number_of_nodes())\n",
        "        ub = ds_k if (ds_k is not None and ds_k > 0) else base_ub\n",
        "        spec_col, spec_k, spec_t = spectral_coloring(noncomm_graph, upper_bound_k=ub, rng=rng)\n",
        "        methods_out['Spectral'] = {'colors': spec_k, 'time': spec_t}\n",
        "    if 'gnn' in selected and need_noncomm:\n",
        "        base_ub = max(2, noncomm_graph.number_of_nodes())\n",
        "        ub = ds_k if (ds_k is not None and ds_k > 0) else base_ub\n",
        "        if gnn_model_path:\n",
        "            try:\n",
        "                gnn_col, gnn_k, gnn_t = color_with_trained_gnn(noncomm_graph, model_path=gnn_model_path, upper_bound_k=ub, rng=rng)\n",
        "                methods_out['GNN'] = {'colors': gnn_k, 'time': gnn_t, 'backend': 'pretrained'}\n",
        "            except Exception as e:\n",
        "                print(f\"[WARN] pretrained GNN failed ({e}); falling back to per-instance training.\")\n",
        "                gnn_col, gnn_k, gnn_t, used = gnn_coloring(noncomm_graph, upper_bound_k=ub, rng=rng,\n",
        "                                                          epochs=gnn_epochs, hidden_dim=gnn_hidden,\n",
        "                                                          emb_dim=gnn_emb, lr=gnn_lr, verbose=False)\n",
        "                methods_out['GNN'] = {'colors': gnn_k, 'time': gnn_t, 'backend': 'torch' if used else 'spectral_fallback'}\n",
        "        else:\n",
        "            gnn_col, gnn_k, gnn_t, used = gnn_coloring(noncomm_graph, upper_bound_k=ub, rng=rng,\n",
        "                                                       epochs=gnn_epochs, hidden_dim=gnn_hidden,\n",
        "                                                       emb_dim=gnn_emb, lr=gnn_lr, verbose=False)\n",
        "            methods_out['GNN'] = {'colors': gnn_k, 'time': gnn_t, 'backend': 'torch' if used else 'spectral_fallback'}\n",
        "    if 'ilp' in selected and need_comm:\n",
        "        ilp_col, ilp_t, ilp_status = solve_ilp_clique_cover(comm_graph)\n",
        "        ilp_k = len(set(ilp_col.values()))\n",
        "        methods_out['ILP'] = {'colors': ilp_k, 'time': ilp_t, 'status': ilp_status}\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== Results (hetero) ===\")\n",
        "        for name in ['DSATUR','RLF','Spectral','GNN','ILP']:\n",
        "            if name in methods_out:\n",
        "                line = f\"{name:8s} colors={methods_out[name].get('colors')} time={methods_out[name].get('time'):.4f}s\"\n",
        "                if name == 'GNN' and 'backend' in methods_out[name]:\n",
        "                    line += f\" backend={methods_out[name]['backend']}\"\n",
        "                if name == 'ILP' and 'status' in methods_out[name]:\n",
        "                    line += f\" status={methods_out[name]['status']}\"\n",
        "                print(line)\n",
        "\n",
        "    return {\n",
        "        'meta': {'nq': nq, 'nt': nt, 'M': M, 'seed': seed, 'methods': sorted(list(selected))},\n",
        "        'graphs': graphs_info,\n",
        "        'methods': methods_out,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2043478e",
      "metadata": {},
      "source": [
        "## Example Run\n",
        "Below cells run a tiny example on a single system and a heterogeneous system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0e3671a",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Operators: M=255, d=2, N=4\n",
            "Comm     edges=16065      build=0.750178s\n",
            "\n",
            "=== Results ===\n",
            "ILP      colors=17 time=3.7448s status=Optimal\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'meta': {'d': 2, 'N': 4, 'M': 255, 'seed': 2, 'methods': ['ilp']},\n",
              " 'graphs': {'comm_edges': 16065,\n",
              "  'noncomm_edges': 0,\n",
              "  'comm_build_time': 0.7501776218414307,\n",
              "  'noncomm_build_time': 0.0},\n",
              " 'methods': {'ILP': {'colors': 17,\n",
              "   'time': 3.744847297668457,\n",
              "   'status': 'Optimal'}}}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example: run_benchmark on a small qubit system (d=2, N=4) with a subset of methods\n",
        "res = run_benchmark(d=2, N=4, M=255, seed=2, methods='ilp', gnn_epochs=50, verbose=True)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "450991b2",
      "metadata": {},
      "source": [
        "  # Quantum Tomography Design — Partitioning & Scheduling\n",
        "\n",
        "This script automates the design of measurement **settings** for quantum state tomography (QST) by partitioning non-identity Pauli strings into **commuting sets** (i.e., settings that can be measured under one basis/unitary). It offers fast heuristics, exact ILP, and hybrid strategies, plus simultaneous diagonalization to derive measurement bases and a **sensory matrix `A`**.\n",
        "\n",
        "---\n",
        "\n",
        "## Highlights\n",
        "\n",
        "- **Operator generation:** All non-identity Pauli strings for `n` qubits.\n",
        "- **Commutation graph:** Nodes = operators; edges = **non-commuting** pairs.\n",
        "- **Partitioning methods:**\n",
        "  - **DSATUR** (fast graph coloring heuristic)\n",
        "  - **RLF** (Recursive Largest First)\n",
        "  - **Spectral clustering** (on complement graph + KMeans)\n",
        "  - **Neural network (GNN)** (probabilistic coloring with conflict loss)\n",
        "  - **ILP (optimal)** (min #settings / feasible `K`-coloring)\n",
        "  - **Combined heuristic + ILP** (binary search over `K`)\n",
        "- **Verification & optimization:** Validates commuting sets and greedily merges compatible groups.\n",
        "- **Measurement unitaries:** Simultaneous diagonalization per group with unitarity checks.\n",
        "- **Sensing matrix `A`:** Built from projectors of eigenvectors; reports SVD, rank, and condition number.\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "```bash\n",
        "# DSATUR heuristic (default)\n",
        "python tomography_design.py 2\n",
        "\n",
        "# Exact ILP (small systems)\n",
        "python tomography_design.py 2 --ilp\n",
        "\n",
        "# Other options\n",
        "python tomography_design.py 4 --rlf\n",
        "python tomography_design.py 3 --spectral\n",
        "python tomography_design.py 5 --nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c9d5f3fe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(CVXPY) Nov 14 02:44:39 AM: Encountered unexpected exception importing solver GLOP:\n",
            "RuntimeError('Unrecognized new version of ortools (9.11.4210). Expected < 9.10.0. Please open a feature request on cvxpy to enable support for this version.')\n",
            "(CVXPY) Nov 14 02:44:39 AM: Encountered unexpected exception importing solver PDLP:\n",
            "RuntimeError('Unrecognized new version of ortools (9.11.4210). Expected < 9.10.0. Please open a feature request on cvxpy to enable support for this version.')\n",
            "\n",
            "Number of non-identity Pauli operators for 3 qubits: 63\n",
            "Expected optimal number of measurement settings: 9\n",
            "Building commutation graph...\n",
            "Commutation graph built.\n",
            "\n",
            "Applying combined heuristic + ILP partitioning...\n",
            "Combined ILP: theoretical lower bound = 9, largest clique = 7, using lower bound = 9, and upper bound = 10\n",
            "Trying K = 9 colors...\n",
            "Feasible with 9 colors.\n",
            "Optimal number of settings found: 9\n",
            "\n",
            "All operators are included in the initial measurement settings.\n",
            "\n",
            "Total measurement settings after initial assignment: 9\n",
            "Optimizing commuting sets by merging compatible sets...\n",
            "Number of measurement settings after optimization: 9\n",
            "Verifying commutativity of final commuting sets...\n",
            "Commutativity Verification: Passed\n",
            "\n",
            "Constructing unitary matrices and sensory matrix A...\n",
            "Measurement Setting 1 Unitarity Check: Passed\n",
            "--------------------------------------------------\n",
            "Measurement Setting 2 Unitarity Check: Passed\n",
            "--------------------------------------------------\n",
            "Measurement Setting 3 Unitarity Check: Passed\n",
            "--------------------------------------------------\n",
            "Measurement Setting 4 Unitarity Check: Passed\n",
            "--------------------------------------------------\n",
            "Measurement Setting 5 Unitarity Check: Passed\n",
            "--------------------------------------------------\n",
            "Measurement Setting 6 Unitarity Check: Passed\n",
            "--------------------------------------------------\n",
            "Measurement Setting 7 Unitarity Check: Passed\n",
            "--------------------------------------------------\n",
            "Measurement Setting 8 Unitarity Check: Passed\n",
            "--------------------------------------------------\n",
            "Measurement Setting 9 Unitarity Check: Passed\n",
            "--------------------------------------------------\n",
            "\n",
            "Sensory matrix A dimensions: (72, 64)\n",
            "Computing Singular Value Decomposition (SVD) for A...\n",
            "SVD computed successfully.\n",
            "Rank of A: 64\n",
            "Condition number of A: 558.4080330865368\n",
            "\n",
            "Total computation time: 7.44 seconds\n",
            "\n",
            "=== Final Results ===\n",
            "Optimized Commuting Sets:\n",
            "  Set 1: [2, 23, 26, 35, 38, 59, 62]\n",
            "  Set 2: [6, 9, 12, 47, 54, 57, 60]\n",
            "  Set 3: [0, 19, 20, 43, 44, 55, 56]\n",
            "  Set 4: [4, 25, 30, 42, 45, 48, 51]\n",
            "  Set 5: [1, 11, 13, 15, 17, 27, 29]\n",
            "  Set 6: [7, 16, 24, 33, 41, 50, 58]\n",
            "  Set 7: [10, 21, 28, 34, 39, 52, 61]\n",
            "  Set 8: [3, 18, 22, 32, 36, 49, 53]\n",
            "  Set 9: [5, 8, 14, 31, 37, 40, 46]\n",
            "\n",
            "Sensory Matrix A shape: (72, 64)\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Quantum Tomography Design Script with Combined Heuristic + ILP Partitioning\n",
        "\n",
        "This script automates the design of quantum tomography measurement settings.\n",
        "It partitions non-identity Pauli operator strings into commuting sets using several\n",
        "methods:\n",
        "    - DSATUR graph coloring (fast heuristic)\n",
        "    - Neural network–based method (fast heuristic)\n",
        "    - ILP-based method (optimal but slow for large systems)\n",
        "    - Spectral clustering method\n",
        "    - Recursive Largest First (RLF) graph coloring\n",
        "    - Combined heuristic + ILP (balances speed and optimality)\n",
        "\n",
        "Usage (command-line):\n",
        "    python tomography_design.py <number_of_qubits> [--nn] [--ilp] [--combined] [--spectral] [--rlf]\n",
        "\n",
        "Example:\n",
        "    python tomography_design.py 2 --combined\n",
        "\n",
        "Dependencies:\n",
        "    numpy, networkx, qutip, pulp, torch, scikit-learn\n",
        "\"\"\"\n",
        "\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "from qutip import Qobj, sigmax, sigmay, sigmaz, qeye, tensor\n",
        "\n",
        "# ILP optimization\n",
        "import pulp\n",
        "\n",
        "# Neural network dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Spectral clustering\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#############################################\n",
        "# Helper Functions for Pauli Operators\n",
        "#############################################\n",
        "\n",
        "def generate_pauli_strings(n_qubits):\n",
        "    \"\"\"\n",
        "    Generate all non-identity Pauli operator strings for a given number of qubits.\n",
        "    Each operator is represented as a tuple of integers:\n",
        "        0 -> I (Identity)\n",
        "        1 -> X\n",
        "        2 -> Y\n",
        "        3 -> Z\n",
        "    Returns:\n",
        "        ops: List of tuples representing the operator.\n",
        "        labels: List of corresponding string labels.\n",
        "    \"\"\"\n",
        "    pauli_map = ['I', 'X', 'Y', 'Z']\n",
        "    single_qubit_ops = [0, 1, 2, 3]\n",
        "    ops, labels = [], []\n",
        "    for combo in itertools.product(single_qubit_ops, repeat=n_qubits):\n",
        "        if any(p != 0 for p in combo):\n",
        "            ops.append(combo)\n",
        "            labels.append(\"\".join(pauli_map[p] for p in combo))\n",
        "    return ops, labels\n",
        "\n",
        "def commute_pauli_strings(p1, p2):\n",
        "    \"\"\"\n",
        "    Check if two Pauli operator strings commute.\n",
        "    They commute if the number of positions where both operators are non-identity\n",
        "    and differ is even.\n",
        "    \"\"\"\n",
        "    anticommute_count = sum(\n",
        "        1 for a, b in zip(p1, p2) if a != 0 and b != 0 and a != b\n",
        "    )\n",
        "    return anticommute_count % 2 == 0\n",
        "\n",
        "def build_commutation_graph(ops):\n",
        "    \"\"\"\n",
        "    Build a graph where each node represents a Pauli operator string.\n",
        "    An edge is added between two nodes if their operators do NOT commute.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "    N = len(ops)\n",
        "    G.add_nodes_from(range(N))\n",
        "    print(\"Building commutation graph...\")\n",
        "    for i in range(N):\n",
        "        for j in range(i + 1, N):\n",
        "            if not commute_pauli_strings(ops[i], ops[j]):\n",
        "                G.add_edge(i, j)\n",
        "    print(\"Commutation graph built.\")\n",
        "    return G\n",
        "\n",
        "def Qobj_pauli_from_tuple(pauli_tuple):\n",
        "    \"\"\"\n",
        "    Convert a tuple representation of a Pauli operator into a Qobj operator.\n",
        "    \"\"\"\n",
        "    pauli_map = [qeye(2), sigmax(), sigmay(), sigmaz()]\n",
        "    ops = [pauli_map[p] for p in pauli_tuple]\n",
        "    return tensor(ops)\n",
        "\n",
        "#############################################\n",
        "# Graph Coloring Heuristics: DSATUR and RLF\n",
        "#############################################\n",
        "\n",
        "def dsatur_coloring(G, random_seed=None):\n",
        "    \"\"\"\n",
        "    DSATUR graph coloring heuristic.\n",
        "    Returns a dictionary mapping node -> color.\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "\n",
        "    coloring = {}\n",
        "    saturation = {node: 0 for node in G.nodes()}\n",
        "    degrees = {node: G.degree(node) for node in G.nodes()}\n",
        "    # Start with the highest-degree node.\n",
        "    nodes_sorted = sorted(G.nodes(), key=lambda x: (-degrees[x], x))\n",
        "    current = nodes_sorted[0]\n",
        "    coloring[current] = 0\n",
        "\n",
        "    # Update saturation for neighbors.\n",
        "    for neighbor in G.neighbors(current):\n",
        "        saturation[neighbor] = 1\n",
        "\n",
        "    uncolored = set(G.nodes()) - {current}\n",
        "    while uncolored:\n",
        "        # Choose node with highest saturation.\n",
        "        max_sat = max(saturation[node] for node in uncolored)\n",
        "        candidates = [node for node in uncolored if saturation[node] == max_sat]\n",
        "        node = random.choice(candidates) if len(candidates) > 1 else candidates[0]\n",
        "        # Assign the smallest available color.\n",
        "        neighbor_colors = {coloring.get(neigh) for neigh in G.neighbors(node) if neigh in coloring}\n",
        "        color = 0\n",
        "        while color in neighbor_colors:\n",
        "            color += 1\n",
        "        coloring[node] = color\n",
        "        # Update saturation for uncolored neighbors.\n",
        "        for neighbor in G.neighbors(node):\n",
        "            if neighbor in uncolored:\n",
        "                neighbor_used_colors = {coloring.get(n) for n in G.neighbors(neighbor) if n in coloring}\n",
        "                saturation[neighbor] = len(neighbor_used_colors)\n",
        "        uncolored.remove(node)\n",
        "    return coloring\n",
        "\n",
        "def find_best_dsatur_coloring(G, attempts=10, fixed_seed=None):\n",
        "    \"\"\"\n",
        "    Run DSATUR multiple times and return the coloring with the fewest colors.\n",
        "    \"\"\"\n",
        "    best_coloring, best_num = None, float('inf')\n",
        "    for attempt in range(attempts):\n",
        "        seed = fixed_seed + attempt if fixed_seed is not None else random.randint(0, 1000000)\n",
        "        coloring = dsatur_coloring(G, random_seed=seed)\n",
        "        num_colors = max(coloring.values()) + 1\n",
        "        if num_colors < best_num:\n",
        "            best_num = num_colors\n",
        "            best_coloring = coloring\n",
        "    return best_coloring\n",
        "\n",
        "def find_dsatur_commuting_partition(G, attempts=10, fixed_seed=None):\n",
        "    \"\"\"\n",
        "    Partition nodes into commuting sets using DSATUR coloring.\n",
        "    Returns a dict mapping color index to list of operator indices.\n",
        "    \"\"\"\n",
        "    coloring = find_best_dsatur_coloring(G, attempts=attempts, fixed_seed=fixed_seed)\n",
        "    partition = {}\n",
        "    for node, color in coloring.items():\n",
        "        partition.setdefault(color, []).append(node)\n",
        "    return partition\n",
        "\n",
        "def rlf_coloring(G, random_seed=None):\n",
        "    \"\"\"\n",
        "    Recursive Largest First (RLF) graph coloring algorithm.\n",
        "    Returns a dictionary mapping each node to a color.\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        random.seed(random_seed)\n",
        "    coloring = {}\n",
        "    color = 0\n",
        "    uncolored = set(G.nodes())\n",
        "    while uncolored:\n",
        "        S = set(uncolored)  # Candidate vertices\n",
        "        independent_set = set()  # Current color class\n",
        "        working_set = set(S)\n",
        "        while True:\n",
        "            if not independent_set:\n",
        "                # Pick vertex with maximum degree in S.\n",
        "                subG = G.subgraph(S)\n",
        "                degrees = {u: subG.degree(u) for u in S}\n",
        "                max_deg = max(degrees.values())\n",
        "                candidates = [u for u, d in degrees.items() if d == max_deg]\n",
        "                v = random.choice(candidates)\n",
        "            else:\n",
        "                # Choose vertex in working_set not adjacent to any in independent_set.\n",
        "                T = {u for u in working_set if all(not G.has_edge(u, w) for w in independent_set)}\n",
        "                if not T:\n",
        "                    break\n",
        "                subG = G.subgraph(S)\n",
        "                degrees = {u: subG.degree(u) for u in T}\n",
        "                max_deg = max(degrees.values())\n",
        "                candidates = [u for u, d in degrees.items() if d == max_deg]\n",
        "                v = random.choice(candidates)\n",
        "            independent_set.add(v)\n",
        "            S.remove(v)\n",
        "            working_set.discard(v)\n",
        "            # Remove neighbors of v from working_set.\n",
        "            working_set.difference_update(set(G.neighbors(v)))\n",
        "        # Assign the current color to all vertices in the independent set.\n",
        "        for v in independent_set:\n",
        "            coloring[v] = color\n",
        "        color += 1\n",
        "        uncolored.difference_update(independent_set)\n",
        "    return coloring\n",
        "\n",
        "def find_rlf_commuting_partition(G, attempts=10, fixed_seed=None):\n",
        "    \"\"\"\n",
        "    Run RLF coloring several times and return the partition corresponding to\n",
        "    the best (fewest colors) coloring found.\n",
        "    Returns a dict mapping color index to list of operator indices.\n",
        "    \"\"\"\n",
        "    best_coloring, best_num = None, float('inf')\n",
        "    for attempt in range(attempts):\n",
        "        seed = fixed_seed + attempt if fixed_seed is not None else random.randint(0, 1000000)\n",
        "        coloring = rlf_coloring(G, random_seed=seed)\n",
        "        num_colors = max(coloring.values()) + 1\n",
        "        if num_colors < best_num:\n",
        "            best_num = num_colors\n",
        "            best_coloring = coloring\n",
        "    partition = {}\n",
        "    for node, col in best_coloring.items():\n",
        "        partition.setdefault(col, []).append(node)\n",
        "    return partition\n",
        "\n",
        "#############################################\n",
        "# ILP-Based Partitioning Functions\n",
        "#############################################\n",
        "\n",
        "def optimal_partition_ilp(ops, G):\n",
        "    \"\"\"\n",
        "    Full ILP formulation for optimal partitioning.\n",
        "    May be slow for larger problems.\n",
        "    Returns a list of groups (each group is a list of operator indices).\n",
        "    \"\"\"\n",
        "    N = len(ops)\n",
        "    M = N  # Worst-case: each operator in its own setting.\n",
        "    prob = pulp.LpProblem(\"OptimalMeasurementSettings\", pulp.LpMinimize)\n",
        "\n",
        "    # Decision variables: x[i, j] indicates operator i is assigned to setting j.\n",
        "    x = pulp.LpVariable.dicts(\"x\", ((i, j) for i in range(N) for j in range(M)), cat=\"Binary\")\n",
        "    # y[j] indicates that setting j is used.\n",
        "    y = pulp.LpVariable.dicts(\"y\", (j for j in range(M)), cat=\"Binary\")\n",
        "\n",
        "    # Each operator must be assigned exactly one setting.\n",
        "    for i in range(N):\n",
        "        prob += pulp.lpSum(x[i, j] for j in range(M)) == 1\n",
        "\n",
        "    # If an operator is assigned to setting j, then setting j is used.\n",
        "    for i in range(N):\n",
        "        for j in range(M):\n",
        "            prob += x[i, j] <= y[j]\n",
        "\n",
        "    # Noncommuting operators cannot share a setting.\n",
        "    for j in range(M):\n",
        "        for i in range(N):\n",
        "            for k in range(i + 1, N):\n",
        "                if not commute_pauli_strings(ops[i], ops[k]):\n",
        "                    prob += x[i, j] + x[k, j] <= 1\n",
        "\n",
        "    # Objective: minimize the number of settings used.\n",
        "    prob += pulp.lpSum(y[j] for j in range(M))\n",
        "    prob.solve()\n",
        "    print(\"Full ILP Status:\", pulp.LpStatus[prob.status])\n",
        "\n",
        "    # Extract partition.\n",
        "    partition = []\n",
        "    for j in range(M):\n",
        "        group = [i for i in range(N)\n",
        "                 if pulp.value(x[i, j]) is not None and pulp.value(x[i, j]) > 0.5]\n",
        "        if group:\n",
        "            partition.append(group)\n",
        "    return partition\n",
        "\n",
        "def ilp_feasible_coloring(ops, G, K):\n",
        "    \"\"\"\n",
        "    ILP feasibility check for K-coloring.\n",
        "    Returns a tuple (feasible, partition), where partition maps color index to operator indices.\n",
        "    \"\"\"\n",
        "    N = len(ops)\n",
        "    prob = pulp.LpProblem(\"FeasibleColoring\", pulp.LpMinimize)\n",
        "\n",
        "    # Binary variables: x[i, c] = 1 if operator i gets color c.\n",
        "    x = pulp.LpVariable.dicts(\"x\", ((i, c) for i in range(N) for c in range(K)), cat=\"Binary\")\n",
        "\n",
        "    # Each operator must be assigned exactly one color.\n",
        "    for i in range(N):\n",
        "        prob += pulp.lpSum(x[i, c] for c in range(K)) == 1\n",
        "\n",
        "    # Noncommuting operators cannot share the same color.\n",
        "    for c in range(K):\n",
        "        for i, j in G.edges():\n",
        "            prob += x[i, c] + x[j, c] <= 1\n",
        "\n",
        "    prob.setObjective(pulp.lpSum([]))  # Trivial objective.\n",
        "    result = prob.solve(pulp.PULP_CBC_CMD(msg=0))\n",
        "    feasible = (pulp.LpStatus[prob.status] == \"Optimal\")\n",
        "    partition = {}\n",
        "    if feasible:\n",
        "        for c in range(K):\n",
        "            partition[c] = [i for i in range(N)\n",
        "                            if pulp.value(x[i, c]) is not None and pulp.value(x[i, c]) > 0.5]\n",
        "    return feasible, partition\n",
        "\n",
        "def combined_optimal_partition(ops, G, heuristic_method=\"dsatur\", fixed_seed=250):\n",
        "    \"\"\"\n",
        "    Combined heuristic + ILP partitioning.\n",
        "    Uses a heuristic (default DSATUR) to obtain an upper bound on the number of settings,\n",
        "    then performs binary search over K (number of settings) using ILP feasibility.\n",
        "    Returns a list of groups (each group is a list of operator indices).\n",
        "    \"\"\"\n",
        "    n_qubits = len(ops[0]) if ops else 1\n",
        "    expected_settings = (4**n_qubits - 1) // (2**n_qubits - 1)\n",
        "\n",
        "    if heuristic_method == \"nn\":\n",
        "        heuristic_partition = neural_network_coloring(G, ops, n_qubits,\n",
        "                                                      num_epochs=100, lr=0.01, lambda_reg=0.1)\n",
        "        upper_bound = len(heuristic_partition)\n",
        "    else:\n",
        "        dsatur_partition = find_dsatur_commuting_partition(G, attempts=10, fixed_seed=fixed_seed)\n",
        "        heuristic_partition = list(dsatur_partition.values())\n",
        "        upper_bound = len(heuristic_partition)\n",
        "\n",
        "    lower_bound = max(largest_clique_size(G), expected_settings)\n",
        "    print(f\"Combined ILP: theoretical lower bound = {expected_settings}, \"\n",
        "          f\"largest clique = {largest_clique_size(G)}, \"\n",
        "          f\"using lower bound = {lower_bound}, and upper bound = {upper_bound}\")\n",
        "\n",
        "    best_K = upper_bound\n",
        "    best_partition = heuristic_partition\n",
        "\n",
        "    # Binary search for smallest feasible K.\n",
        "    left, right = lower_bound, upper_bound\n",
        "    while left <= right:\n",
        "        mid = (left + right) // 2\n",
        "        print(f\"Trying K = {mid} colors...\")\n",
        "        feasible, partition = ilp_feasible_coloring(ops, G, mid)\n",
        "        if feasible:\n",
        "            best_K = mid\n",
        "            best_partition = [grp for grp in partition.values() if grp]\n",
        "            right = mid - 1\n",
        "            print(f\"Feasible with {mid} colors.\")\n",
        "        else:\n",
        "            left = mid + 1\n",
        "            print(f\"Not feasible with {mid} colors.\")\n",
        "    print(f\"Optimal number of settings found: {best_K}\")\n",
        "    return best_partition\n",
        "\n",
        "def largest_clique_size(G):\n",
        "    \"\"\"\n",
        "    Return the size of the largest clique in the graph.\n",
        "    \"\"\"\n",
        "    cliques = list(nx.find_cliques(G))\n",
        "    return max((len(clique) for clique in cliques), default=1)\n",
        "\n",
        "#############################################\n",
        "# Neural Network (GNN) for Graph Coloring\n",
        "#############################################\n",
        "\n",
        "def pauli_to_feature(pauli_tuple):\n",
        "    \"\"\"\n",
        "    Convert a Pauli operator tuple to a one-hot encoded feature vector.\n",
        "    \"\"\"\n",
        "    feature = []\n",
        "    for p in pauli_tuple:\n",
        "        one_hot = [0, 0, 0, 0]\n",
        "        one_hot[p] = 1\n",
        "        feature.extend(one_hot)\n",
        "    return feature\n",
        "\n",
        "def generate_features(ops):\n",
        "    \"\"\"\n",
        "    Generate a feature matrix (each row is the one-hot encoding of an operator).\n",
        "    \"\"\"\n",
        "    features = [pauli_to_feature(op) for op in ops]\n",
        "    return np.array(features, dtype=np.float32)\n",
        "\n",
        "class GCNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple Graph Convolutional Layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GCNLayer, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, X, A_hat):\n",
        "        support = self.linear(X)\n",
        "        out = torch.matmul(A_hat, support)\n",
        "        return F.relu(out)\n",
        "\n",
        "class GNNColoringModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Two-layer GNN that outputs logits for a fixed number of colors.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features, num_colors):\n",
        "        super(GNNColoringModel, self).__init__()\n",
        "        self.gcn1 = GCNLayer(in_features, hidden_features)\n",
        "        self.gcn2 = GCNLayer(hidden_features, hidden_features)\n",
        "        self.fc = nn.Linear(hidden_features, num_colors)\n",
        "\n",
        "    def forward(self, X, A_hat):\n",
        "        x = self.gcn1(X, A_hat)\n",
        "        x = self.gcn2(x, A_hat)\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "def refine_nn_partition(ops, partition):\n",
        "    \"\"\"\n",
        "    Refine a partition so that each group contains only mutually commuting operators.\n",
        "    Accepts partition as a dict or list of lists.\n",
        "    \"\"\"\n",
        "    groups = list(partition.values()) if isinstance(partition, dict) else partition\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        new_groups = []\n",
        "        for group in groups:\n",
        "            valid_group, leftovers = [], []\n",
        "            for node in group:\n",
        "                if all(commute_pauli_strings(ops[node], ops[other]) for other in valid_group):\n",
        "                    valid_group.append(node)\n",
        "                else:\n",
        "                    leftovers.append(node)\n",
        "            new_groups.append(valid_group)\n",
        "            for node in leftovers:\n",
        "                assigned = False\n",
        "                for grp in new_groups:\n",
        "                    if all(commute_pauli_strings(ops[node], ops[other]) for other in grp):\n",
        "                        grp.append(node)\n",
        "                        assigned = True\n",
        "                        break\n",
        "                if not assigned:\n",
        "                    new_groups.append([node])\n",
        "                    changed = True\n",
        "        groups = new_groups\n",
        "    return groups\n",
        "\n",
        "def neural_network_coloring(G, ops, n_qubits, num_epochs=200, lr=0.01,\n",
        "                            num_colors=None, max_graph_size=500, lambda_reg=0.1):\n",
        "    \"\"\"\n",
        "    GNN-based graph coloring. If the graph is large, it partitions it into chunks.\n",
        "    Returns a partition (list of groups of operator indices) refined so that each group is commuting.\n",
        "    \"\"\"\n",
        "    N = len(ops)\n",
        "    # If graph too large, process in chunks.\n",
        "    if N > max_graph_size:\n",
        "        print(f\"Graph has {N} nodes (>{max_graph_size}). Partitioning into chunks...\")\n",
        "        node_list = list(G.nodes())\n",
        "        chunks = [node_list[i:i+max_graph_size] for i in range(0, N, max_graph_size)]\n",
        "        partitions = []\n",
        "        for chunk in chunks:\n",
        "            subG = G.subgraph(chunk).copy()\n",
        "            # FIX: Re-label subgraph nodes to consecutive integers.\n",
        "            subG = nx.convert_node_labels_to_integers(subG)\n",
        "            sub_ops = [ops[i] for i in chunk]\n",
        "            part_chunk = neural_network_coloring(subG, sub_ops, n_qubits,\n",
        "                                                 num_epochs=num_epochs, lr=lr,\n",
        "                                                 num_colors=num_colors, max_graph_size=max_graph_size,\n",
        "                                                 lambda_reg=lambda_reg)\n",
        "            # Map back to original indices.\n",
        "            for group in part_chunk:\n",
        "                original_group = [chunk[i] for i in group]\n",
        "                partitions.append(original_group)\n",
        "        return partitions\n",
        "\n",
        "    # Prepare features and adjacency.\n",
        "    features = generate_features(ops)  # Shape: (N, n_qubits * 4)\n",
        "    N, in_features = features.shape\n",
        "    if num_colors is None:\n",
        "        expected_settings = (4**n_qubits - 1) // (2**n_qubits - 1)\n",
        "        num_colors = expected_settings * 2\n",
        "\n",
        "    # Build normalized adjacency.\n",
        "    A = nx.adjacency_matrix(G).todense()\n",
        "    A = np.array(A, dtype=np.float32)\n",
        "    A += np.eye(N, dtype=np.float32)\n",
        "    D = np.sum(A, axis=1)\n",
        "    D_inv_sqrt = np.diag(1.0 / np.sqrt(D))\n",
        "    A_hat = D_inv_sqrt @ A @ D_inv_sqrt\n",
        "\n",
        "    # Convert to torch tensors.\n",
        "    X = torch.tensor(features)\n",
        "    A_hat_t = torch.tensor(A_hat)\n",
        "    hidden_features = 32\n",
        "    model = GNNColoringModel(in_features, hidden_features, num_colors)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    edges = list(G.edges())\n",
        "\n",
        "    # Train the model.\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X, A_hat_t)\n",
        "        P = F.softmax(logits, dim=1)\n",
        "        # Conflict loss: penalize adjacent nodes sharing probability mass.\n",
        "        conflict_loss = sum(torch.sum(P[i] * P[j]) for i, j in edges) / len(edges)\n",
        "        # Regularization to encourage balanced color usage.\n",
        "        usage = P.mean(dim=0)\n",
        "        usage_entropy = -torch.sum(usage * torch.log(usage + 1e-10))\n",
        "        loss = conflict_loss + lambda_reg * usage_entropy\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 50 == 0:\n",
        "            print(f\"[NN Coloring] Epoch {epoch}, Conflict Loss: {conflict_loss.item():.4f}, \"\n",
        "                  f\"Usage Entropy: {usage_entropy.item():.4f}, Total Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Get color assignments.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X, A_hat_t)\n",
        "        predicted_colors = torch.argmax(logits, dim=1).numpy().tolist()\n",
        "    partition = {}\n",
        "    for node, color in enumerate(predicted_colors):\n",
        "        partition.setdefault(color, []).append(node)\n",
        "    refined_partition = refine_nn_partition(ops, partition)\n",
        "    return refined_partition\n",
        "\n",
        "#############################################\n",
        "# Spectral Clustering for Graph Coloring\n",
        "#############################################\n",
        "\n",
        "def spectral_coloring(G, ops, n_qubits, num_clusters=None):\n",
        "    \"\"\"\n",
        "    Spectral clustering–based coloring.\n",
        "    Steps:\n",
        "      1. Compute the complement graph (nodes connected if they commute).\n",
        "      2. Form the normalized Laplacian.\n",
        "      3. Compute eigenvectors corresponding to smallest eigenvalues.\n",
        "      4. Cluster rows of eigenvector matrix using KMeans.\n",
        "      5. Refine clusters so that each group is mutually commuting.\n",
        "    Returns a refined partition (list of groups of operator indices).\n",
        "    \"\"\"\n",
        "    # Complement graph: connection indicates commutation.\n",
        "    G_compl = nx.complement(G)\n",
        "    A = nx.adjacency_matrix(G_compl).todense()\n",
        "    A = np.array(A, dtype=np.float32)\n",
        "    D = np.diag(np.array(A.sum(axis=1)).flatten())\n",
        "    D_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(D) + 1e-10))\n",
        "    L = np.eye(A.shape[0]) - D_inv_sqrt @ A @ D_inv_sqrt\n",
        "\n",
        "    # Determine number of clusters.\n",
        "    if num_clusters is None:\n",
        "        expected_settings = (4**n_qubits - 1) // (2**n_qubits - 1)\n",
        "        dsatur_col = find_best_dsatur_coloring(G, attempts=10)\n",
        "        dsatur_num = max(dsatur_col.values()) + 1\n",
        "        num_clusters = min(expected_settings, dsatur_num)\n",
        "        num_clusters = max(num_clusters, 1)\n",
        "\n",
        "    eigenvals, eigenvecs = np.linalg.eigh(L)\n",
        "    X = eigenvecs[:, :num_clusters]\n",
        "    kmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "\n",
        "    partition = {}\n",
        "    for idx, label in enumerate(labels):\n",
        "        partition.setdefault(label, []).append(idx)\n",
        "\n",
        "    refined_partition = refine_nn_partition(ops, partition)\n",
        "    return refined_partition\n",
        "\n",
        "#############################################\n",
        "# Simultaneous Diagonalization Helpers\n",
        "#############################################\n",
        "\n",
        "def unique_eigvals(eigvals, tol=1e-12):\n",
        "    \"\"\"\n",
        "    Group eigenvalues that are identical within a tolerance.\n",
        "    Returns a list of groups.\n",
        "    \"\"\"\n",
        "    sorted_vals = np.sort(eigvals)\n",
        "    groups = []\n",
        "    used = np.zeros(len(sorted_vals), dtype=bool)\n",
        "    for i, val in enumerate(sorted_vals):\n",
        "        if used[i]:\n",
        "            continue\n",
        "        group = [val]\n",
        "        used[i] = True\n",
        "        for j in range(i + 1, len(sorted_vals)):\n",
        "            if not used[j] and abs(sorted_vals[j] - val) < tol:\n",
        "                used[j] = True\n",
        "                group.append(sorted_vals[j])\n",
        "        groups.append(group)\n",
        "    return groups\n",
        "\n",
        "def refine_basis(op, basis_vecs):\n",
        "    \"\"\"\n",
        "    Diagonalize operator op within a given subspace defined by basis_vecs.\n",
        "    Returns eigenvalues and the refined basis.\n",
        "    \"\"\"\n",
        "    proj_op = basis_vecs.conj().T @ op.full() @ basis_vecs\n",
        "    w, U = np.linalg.eigh(proj_op)\n",
        "    new_basis = basis_vecs @ U\n",
        "    return w, new_basis\n",
        "\n",
        "def simultaneous_eig_general(operators):\n",
        "    \"\"\"\n",
        "    Simultaneously diagonalize a set of commuting operators.\n",
        "    Returns a matrix whose columns form the final basis.\n",
        "    \"\"\"\n",
        "    if not operators:\n",
        "        raise ValueError(\"No operators provided for simultaneous diagonalization.\")\n",
        "    eigvals, eigstates = operators[0].eigenstates()\n",
        "    eigvals = np.array(eigvals)\n",
        "    V = np.column_stack([st.full().flatten() for st in eigstates])\n",
        "    Q, _ = np.linalg.qr(V)\n",
        "    final_basis = Q\n",
        "    final_eigvals = eigvals\n",
        "    for op in operators[1:]:\n",
        "        new_vectors = []\n",
        "        new_eigvals = []\n",
        "        for group in unique_eigvals(final_eigvals):\n",
        "            indices = [i for i, val in enumerate(final_eigvals)\n",
        "                       if any(abs(val - x) < 1e-12 for x in group)]\n",
        "            subspace = final_basis[:, indices]\n",
        "            if subspace.shape[1] > 1:\n",
        "                w_sub, refined = refine_basis(op, subspace)\n",
        "                new_vectors.append(refined)\n",
        "                new_eigvals.extend(w_sub)\n",
        "            else:\n",
        "                new_vectors.append(subspace)\n",
        "                new_eigvals.append(final_eigvals[indices[0]])\n",
        "        final_basis = np.hstack(new_vectors)\n",
        "        Q_final, _ = np.linalg.qr(final_basis)\n",
        "        final_basis = Q_final\n",
        "        final_eigvals = np.array(new_eigvals)\n",
        "    return final_basis\n",
        "\n",
        "def verify_commuting_sets(ops, commuting_sets):\n",
        "    \"\"\"\n",
        "    Verify that all operators within each set commute.\n",
        "    Returns True if valid; prints an error message and returns False otherwise.\n",
        "    \"\"\"\n",
        "    for s in commuting_sets:\n",
        "        for i, j in itertools.combinations(s, 2):\n",
        "            if not commute_pauli_strings(ops[i], ops[j]):\n",
        "                op_i = \"\".join(\"IXYZ\"[p] for p in ops[i])\n",
        "                op_j = \"\".join(\"IXYZ\"[p] for p in ops[j])\n",
        "                print(f\"Operators {i} ({op_i}) and {j} ({op_j}) do not commute!\")\n",
        "                return False\n",
        "    return True\n",
        "\n",
        "def optimize_commuting_sets(ops, commuting_sets):\n",
        "    \"\"\"\n",
        "    Greedily merge compatible sets to further reduce the number of measurement settings.\n",
        "    Returns the optimized list of commuting sets.\n",
        "    \"\"\"\n",
        "    sorted_sets = sorted(commuting_sets, key=lambda s: len(s))\n",
        "    optimized_sets = sorted_sets.copy()\n",
        "    for small_set in sorted_sets:\n",
        "        if not small_set:\n",
        "            continue\n",
        "        for op in small_set.copy():\n",
        "            for target_set in optimized_sets:\n",
        "                if target_set is small_set:\n",
        "                    continue\n",
        "                if all(commute_pauli_strings(ops[op], ops[j]) for j in target_set):\n",
        "                    target_set.append(op)\n",
        "                    small_set.remove(op)\n",
        "                    break\n",
        "    return [s for s in optimized_sets if s]\n",
        "\n",
        "#############################################\n",
        "# Main Tomography Design Function\n",
        "#############################################\n",
        "\n",
        "def run_tomography_design(n_qubits, dsatur_attempts=25, fixed_seed=250,\n",
        "                          use_nn=False, use_ilp=False, use_combined=False,\n",
        "                          use_spectral=False, use_rlf=False):\n",
        "    \"\"\"\n",
        "    Execute the tomography design process.\n",
        "    Chooses partitioning method based on flags:\n",
        "      - use_combined: combined heuristic + ILP approach.\n",
        "      - use_ilp: full ILP method.\n",
        "      - use_nn: neural network method.\n",
        "      - use_spectral: spectral clustering method.\n",
        "      - use_rlf: Recursive Largest First method.\n",
        "      - Otherwise, defaults to DSATUR.\n",
        "    Returns:\n",
        "      optimized_commuting_sets: List of commuting sets (each is a list of operator indices).\n",
        "      A: Sensory matrix constructed from measurement bases.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    random.seed(fixed_seed)\n",
        "    np.random.seed(fixed_seed)\n",
        "\n",
        "    # Generate Pauli operators.\n",
        "    ops, labels = generate_pauli_strings(n_qubits)\n",
        "    N = len(ops)\n",
        "    expected_settings = (4**n_qubits - 1) // (2**n_qubits - 1)\n",
        "    print(f\"\\nNumber of non-identity Pauli operators for {n_qubits} qubits: {N}\")\n",
        "    print(f\"Expected optimal number of measurement settings: {expected_settings}\")\n",
        "\n",
        "    # Build the commutation graph.\n",
        "    G = build_commutation_graph(ops)\n",
        "\n",
        "    # Select partitioning method.\n",
        "    if use_combined:\n",
        "        print(\"\\nApplying combined heuristic + ILP partitioning...\")\n",
        "        initial_partition = combined_optimal_partition(ops, G, heuristic_method=\"dsatur\", fixed_seed=fixed_seed)\n",
        "    elif use_ilp:\n",
        "        print(\"\\nApplying full ILP-based optimal partitioning...\")\n",
        "        initial_partition = optimal_partition_ilp(ops, G)\n",
        "    elif use_nn:\n",
        "        print(\"\\nApplying Neural Network based coloring...\")\n",
        "        initial_partition = neural_network_coloring(G, ops, n_qubits, max_graph_size=500, lambda_reg=0.1)\n",
        "        # Ensure partition is a list of groups.\n",
        "        if isinstance(initial_partition, dict) or (initial_partition and isinstance(initial_partition[0], int)):\n",
        "            initial_partition = [initial_partition]\n",
        "    elif use_spectral:\n",
        "        print(\"\\nApplying Spectral Clustering based coloring...\")\n",
        "        initial_partition = spectral_coloring(G, ops, n_qubits)\n",
        "    elif use_rlf:\n",
        "        print(\"\\nApplying Recursive Largest First (RLF) graph coloring...\")\n",
        "        rlf_color_map = find_rlf_commuting_partition(G, attempts=dsatur_attempts, fixed_seed=fixed_seed)\n",
        "        initial_partition = list(rlf_color_map.values())\n",
        "    else:\n",
        "        print(\"\\nApplying DSATUR graph coloring...\")\n",
        "        dsatur_color_map = find_dsatur_commuting_partition(G, attempts=dsatur_attempts, fixed_seed=fixed_seed)\n",
        "        initial_partition = list(dsatur_color_map.values())\n",
        "\n",
        "    current_partition = initial_partition\n",
        "\n",
        "    # Ensure all operators are assigned.\n",
        "    all_measured = set(itertools.chain.from_iterable(current_partition))\n",
        "    missing_ops = set(range(N)) - all_measured\n",
        "    if missing_ops:\n",
        "        print(f\"\\nWarning: {len(missing_ops)} operators missing from initial settings.\")\n",
        "        for op_idx in list(missing_ops):\n",
        "            for group in current_partition:\n",
        "                if all(commute_pauli_strings(ops[op_idx], ops[j]) for j in group):\n",
        "                    group.append(op_idx)\n",
        "                    break\n",
        "            else:\n",
        "                # If still unassigned, create a new group.\n",
        "                current_partition.append([op_idx])\n",
        "        print(\"All missing operators have been assigned.\")\n",
        "    else:\n",
        "        print(\"\\nAll operators are included in the initial measurement settings.\")\n",
        "\n",
        "    print(f\"\\nTotal measurement settings after initial assignment: {len(current_partition)}\")\n",
        "    print(\"Optimizing commuting sets by merging compatible sets...\")\n",
        "    optimized_commuting_sets = optimize_commuting_sets(ops, current_partition)\n",
        "    print(f\"Number of measurement settings after optimization: {len(optimized_commuting_sets)}\")\n",
        "\n",
        "    # Verify commuting property.\n",
        "    print(\"Verifying commutativity of final commuting sets...\")\n",
        "    valid = verify_commuting_sets(ops, optimized_commuting_sets)\n",
        "    print(\"Commutativity Verification:\", \"Passed\" if valid else \"Failed\")\n",
        "\n",
        "    # Construct the sensory matrix A.\n",
        "    print(\"\\nConstructing unitary matrices and sensory matrix A...\")\n",
        "    A_rows = []\n",
        "    for idx, group in enumerate(optimized_commuting_sets):\n",
        "        set_ops = [Qobj_pauli_from_tuple(ops[i]) for i in group]\n",
        "        final_basis = simultaneous_eig_general(set_ops)\n",
        "        U = Qobj(final_basis, dims=[[2] * n_qubits, [2] * n_qubits])\n",
        "        # Check unitarity.\n",
        "        check_identity = (U.dag() * U - qeye([2] * n_qubits)).norm() < 1e-12\n",
        "        print(f\"Measurement Setting {idx + 1} Unitarity Check:\", \"Passed\" if check_identity else \"Failed\")\n",
        "        print(\"-\" * 50)\n",
        "        # Construct rows from projector vectors.\n",
        "        for i in range(final_basis.shape[1]):\n",
        "            psi = final_basis[:, i]\n",
        "            P = np.outer(psi, psi.conj())\n",
        "            A_rows.append(P.flatten(order='F').conj())\n",
        "    A = np.array(A_rows, dtype=complex)\n",
        "    print(\"\\nSensory matrix A dimensions:\", A.shape)\n",
        "\n",
        "    # Compute SVD of A.\n",
        "    print(\"Computing Singular Value Decomposition (SVD) for A...\")\n",
        "    try:\n",
        "        U_svd, s_vals, Vh_svd = np.linalg.svd(A, full_matrices=False)\n",
        "        tol = max(A.shape) * np.amax(s_vals) * np.finfo(s_vals.dtype).eps\n",
        "        rank_A = np.sum(s_vals > tol)\n",
        "        cond_A = s_vals[0] / s_vals[-1] if s_vals[-1] > tol else np.inf\n",
        "        print(\"SVD computed successfully.\")\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        print(\"SVD computation failed:\", e)\n",
        "        rank_A = \"Undefined\"\n",
        "        cond_A = \"Undefined\"\n",
        "    print(\"Rank of A:\", rank_A)\n",
        "    print(\"Condition number of A:\", cond_A)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTotal computation time: {total_time:.2f} seconds\")\n",
        "    return optimized_commuting_sets, A\n",
        "\n",
        "#############################################\n",
        "# Main Entry Point\n",
        "#############################################\n",
        "\n",
        "def main_entry():\n",
        "    \"\"\"\n",
        "    Entry point for the tomography design script.\n",
        "    Accepts command-line arguments to select the partitioning method.\n",
        "    Flags:\n",
        "      --nn       : Use Neural Network method.\n",
        "      --ilp      : Use full ILP method.\n",
        "      --combined : Use combined heuristic + ILP method.\n",
        "      --spectral : Use Spectral Clustering method.\n",
        "      --rlf      : Use Recursive Largest First method.\n",
        "    \"\"\"\n",
        "    # Interactive mode (e.g., in a shell) or command-line mode.\n",
        "    if hasattr(sys, 'ps1'):\n",
        "        try:\n",
        "            n_qubits = int(input(\"Enter the number of qubits: \"))\n",
        "            if n_qubits < 1:\n",
        "                raise ValueError\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a positive integer.\")\n",
        "            return\n",
        "        method = input(\"Choose method - (d)SATUR, (n) Neural Network, (i) ILP, (c) Combined, (s) Spectral, or (r) RLF: \").strip().lower()\n",
        "        use_nn = (method == 'n')\n",
        "        use_ilp = (method == 'i')\n",
        "        use_combined = (method == 'c')\n",
        "        use_spectral = (method == 's')\n",
        "        use_rlf = (method == 'r')\n",
        "    else:\n",
        "        if len(sys.argv) < 2:\n",
        "            print(\"Usage: python tomography_design.py <number_of_qubits> [--nn] [--ilp] [--combined] [--spectral] [--rlf]\")\n",
        "            sys.exit(1)\n",
        "        try:\n",
        "            n_qubits = int(sys.argv[1])\n",
        "            if n_qubits < 1:\n",
        "                raise ValueError\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Provide a positive integer for the number of qubits.\")\n",
        "            sys.exit(1)\n",
        "        use_nn = '--nn' in sys.argv[2:]\n",
        "        use_ilp = '--ilp' in sys.argv[2:]\n",
        "        use_combined = '--combined' in sys.argv[2:]\n",
        "        use_spectral = '--spectral' in sys.argv[2:]\n",
        "        use_rlf = '--rlf' in sys.argv[2:]\n",
        "\n",
        "    optimized_commuting_sets, A = run_tomography_design(\n",
        "        n_qubits,\n",
        "        use_nn=use_nn,\n",
        "        use_ilp=use_ilp,\n",
        "        use_combined=use_combined,\n",
        "        use_spectral=use_spectral,\n",
        "        use_rlf=use_rlf\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Final Results ===\")\n",
        "    print(\"Optimized Commuting Sets:\")\n",
        "    for idx, group in enumerate(optimized_commuting_sets, start=1):\n",
        "        print(f\"  Set {idx}: {group}\")\n",
        "    print(\"\\nSensory Matrix A shape:\", A.shape)\n",
        "    # Optionally, print the sensory matrix:\n",
        "    # print(\"Sensory Matrix A:\\n\", A)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_entry()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618778a4",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
